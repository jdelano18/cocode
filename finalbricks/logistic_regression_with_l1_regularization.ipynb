{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["---\n", "title: \"Logistic Regression With L1 Regularization\"\n", "description: \"\"\n", "tags: machine_learning, logistic_regression\n", "URL: https://github.com/chrisalbon/notes\n", "Licence: \n", "Creator: \n", "Meta: \n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": [" <div>\n    \t<img src=\"./coco.png\" style=\"float: left;height: 55px\">\n    \t<div style=\"height: 75px;text-align: center; padding-top:5px\">\n        <h1>\n      \tLogistic Regression With L1 Regularization\n        </h1>\n        <p></p>\n    \t</div>\n\t\t</div> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["L1 regularization (also called least absolute deviations) is a powerful tool in data science. There are many tutorials out there explaining L1 regularization and I will not try to do that here. Instead, this tutorial is show the effect of the regularization parameter `C` on the coefficients and model accuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preliminaries"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn import datasets\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import StandardScaler"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create The Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The dataset used in this tutorial is the famous [iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set). The Iris target data contains 50 samples from three species of Iris, `y` and four feature variables, `X`.\n", "\n", "The dataset contains three categories (three species of Iris), however for the sake of simplicity it is easier if the target data is binary. Therefore we will remove the data from the last species of Iris."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# Load the iris dataset\n", "iris = datasets.load_iris()\n", "\n", "# Create X from the features\n", "X = iris.data\n", "\n", "# Create y from output\n", "y = iris.target\n", "\n", "# Remake the variable, keeping all data where the category is not 2.\n", "X = X[y != 2]\n", "y = y[y != 2]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## View The Data"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[5.1, 3.5, 1.4, 0.2],\n", "       [4.9, 3. , 1.4, 0.2],\n", "       [4.7, 3.2, 1.3, 0.2],\n", "       [4.6, 3.1, 1.5, 0.2],\n", "       [5. , 3.6, 1.4, 0.2]])"]}, "execution_count": 3, "metadata": {}, "output_type": "execute_result"}], "source": ["# View the features\n", "X[0:5]"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n", "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n", "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n", "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n", "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["# View the target data\n", "y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Split The Data Into Training And Test Sets"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["# Split the data into test and training sets, with 30% of samples being put into the test set\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Standardize Features"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Because the regularization penalty is comprised of the sum of the absolute value of the coefficients, we need to scale the data so the coefficients are all based on the same scale."]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# Create a scaler object\n", "sc = StandardScaler()\n", "\n", "# Fit the scaler to the training data and transform\n", "X_train_std = sc.fit_transform(X_train)\n", "\n", "# Apply the scaler to the test data\n", "X_test_std = sc.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Run Logistic Regression With A L1 Penalty With Various Regularization Strengths"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The usefulness of L1 is that it can push feature coefficients to 0, creating a method for feature selection. In the code below we run a logistic regression with a L1 penalty four times, each time decreasing the value of `C`. We should expect that as `C` decreases, more coefficients become 0."]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["C: 10\n", "Coefficient of each feature: [[-0.00902649 -3.83902983  4.34904293  0.        ]]\n", "Training accuracy: 0.9857142857142858\n", "Test accuracy: 1.0\n", "\n", "C: 1\n", "Coefficient of each feature: [[ 0.         -2.27441684  2.56760315  0.        ]]\n", "Training accuracy: 0.9857142857142858\n", "Test accuracy: 1.0\n", "\n", "C: 0.1\n", "Coefficient of each feature: [[ 0.         -0.82143435  0.97187285  0.        ]]\n", "Training accuracy: 0.9857142857142858\n", "Test accuracy: 1.0\n", "\n", "C: 0.001\n", "Coefficient of each feature: [[0. 0. 0. 0.]]\n", "Training accuracy: 0.5\n", "Test accuracy: 0.5\n", "\n"]}], "source": ["C = [10, 1, .1, .001]\n", "\n", "for c in C:\n", "    clf = LogisticRegression(penalty='l1', C=c, solver='liblinear')\n", "    clf.fit(X_train, y_train)\n", "    print('C:', c)\n", "    print('Coefficient of each feature:', clf.coef_)\n", "    print('Training accuracy:', clf.score(X_train_std, y_train))\n", "    print('Test accuracy:', clf.score(X_test_std, y_test))\n", "    print('')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Notice that as `C` decreases the model coefficients become smaller (for example from `4.36276075` when `C=10` to `0.0.97175097` when `C=0.1`), until at `C=0.001` all the coefficients are zero. This is the effect of the regularization penalty becoming more prominent."]}], "metadata": {"anaconda-cloud": {}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.1"}}, "nbformat": 4, "nbformat_minor": 1}