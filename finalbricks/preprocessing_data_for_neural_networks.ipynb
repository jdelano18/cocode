{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["---\n", "title: \"Preprocessing Data For Neural Networks\"\n", "description: \"\"\n", "tags: deep_learning, keras\n", "URL: https://github.com/chrisalbon/notes\n", "Licence: \n", "Creator: \n", "Meta: \n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": [" <div>\n    \t<img src=\"./coco.png\" style=\"float: left;height: 55px\">\n    \t<div style=\"height: 75px;text-align: center; padding-top:5px\">\n        <h1>\n      \tPreprocessing Data For Neural Networks\n        </h1>\n        <p></p>\n    \t</div>\n\t\t</div> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Typically, a neural network's parameters are initialized (i.e. created) as small random numbers. Neural networks often behave poorly when the feature values much larger than parameter values. Furthermore, since an observation's feature values will are combined as they pass through individual units, it is important that all features have the same scale.\n", "\n", "For these reasons, it is best practice (although not always necessary, for example when we have all binary features) to standardize each feature such that the feature's values have the mean of 0 and the standard deviation of 1. This can be easily accomplished using scikit-learn's `StandardScaler`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preliminaries"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# Load libraries\n", "from sklearn import preprocessing\n", "import numpy as np"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create Feature Data"]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["# Create feature\n", "features = np.array([[-100.1, 3240.1], \n", "                     [-200.2, -234.1], \n", "                     [5000.5, 150.1], \n", "                     [6000.6, -125.1], \n", "                     [9000.9, -673.1]])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Standardize Feature Data"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Create scaler\n", "scaler = preprocessing.StandardScaler()\n", "\n", "# Transform the feature\n", "features_standardized = scaler.fit_transform(features)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Show Standardized Features"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[-1.12541308,  1.96429418],\n", "       [-1.15329466, -0.50068741],\n", "       [ 0.29529406, -0.22809346],\n", "       [ 0.57385917, -0.42335076],\n", "       [ 1.40955451, -0.81216255]])"]}, "execution_count": 4, "metadata": {}, "output_type": "execute_result"}], "source": ["# Show feature\n", "features_standardized"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Show Standardized Features Summary Statistics"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Mean: 0.0\n", "Standard deviation: 1.0\n"]}], "source": ["# Print mean and standard deviation\n", "print('Mean:', round(features_standardized[:,0].mean()))\n", "print('Standard deviation:', features_standardized[:,0].std())"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 2}