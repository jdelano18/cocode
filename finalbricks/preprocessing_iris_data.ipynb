{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["---\n", "title: \"Preprocessing Iris Data\"\n", "description: \"\"\n", "tags: machine_learning, preprocessing_structured_data\n", "URL: https://github.com/chrisalbon/notes\n", "Licence: \n", "Creator: \n", "Meta: \n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": [" <div>\n    \t<img src=\"./coco.png\" style=\"float: left;height: 55px\">\n    \t<div style=\"height: 75px;text-align: center; padding-top:5px\">\n        <h1>\n      \tPreprocessing Iris Data\n        </h1>\n        <p></p>\n    \t</div>\n\t\t</div> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preliminaries"]}, {"cell_type": "code", "execution_count": 45, "metadata": {}, "outputs": [], "source": ["from sklearn import datasets\n", "import numpy as np\n", "from sklearn.cross_validation import train_test_split\n", "from sklearn.preprocessing import StandardScaler"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load Data"]}, {"cell_type": "code", "execution_count": 40, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Load the iris data\n", "iris = datasets.load_iris()\n", "\n", "# Create a variable for the feature data\n", "X = iris.data\n", "\n", "# Create a variable for the target data\n", "y = iris.target"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Split Data For Cross Validation"]}, {"cell_type": "code", "execution_count": 47, "metadata": {}, "outputs": [], "source": ["# Random split the data into four new datasets, training features, training outcome, test features, \n", "# and test outcome. Set the size of the test data to be 30% of the full dataset.\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Standardize Feature Data"]}, {"cell_type": "code", "execution_count": 42, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Load the standard scaler\n", "sc = StandardScaler()\n", "\n", "# Compute the mean and standard deviation based on the training data\n", "sc.fit(X_train)\n", "\n", "# Scale the training data to be of mean 0 and of unit variance\n", "X_train_std = sc.transform(X_train)\n", "\n", "# Scale the test data to be of mean 0 and of unit variance\n", "X_test_std = sc.transform(X_test)"]}, {"cell_type": "code", "execution_count": 43, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[ 6.1,  2.8,  4.7,  1.2],\n", "       [ 5.7,  3.8,  1.7,  0.3],\n", "       [ 7.7,  2.6,  6.9,  2.3],\n", "       [ 6. ,  2.9,  4.5,  1.5],\n", "       [ 6.8,  2.8,  4.8,  1.4]])"]}, "execution_count": 43, "metadata": {}, "output_type": "execute_result"}], "source": ["# Feature Test Data, non-standardized\n", "X_test[0:5]"]}, {"cell_type": "code", "execution_count": 44, "metadata": {}, "outputs": [{"data": {"text/plain": ["array([[ 0.3100623 , -0.49582097,  0.48403749, -0.05143998],\n", "       [-0.17225683,  1.92563026, -1.26851205, -1.26670948],\n", "       [ 2.23933883, -0.98011121,  1.76924049,  1.43388941],\n", "       [ 0.18948252, -0.25367584,  0.36720086,  0.35364985],\n", "       [ 1.15412078, -0.49582097,  0.54245581,  0.21861991]])"]}, "execution_count": 44, "metadata": {}, "output_type": "execute_result"}], "source": ["# Feature Test Data, standardized.\n", "X_test_std[0:5]"]}], "metadata": {"kernelspec": {"display_name": "Python [conda root]", "language": "python", "name": "conda-root-py"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.5.3"}}, "nbformat": 4, "nbformat_minor": 1}