{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["---\n", "title: \"Save Model Training Progress\"\n", "description: \"\"\n", "tags: deep_learning, keras\n", "URL: https://github.com/chrisalbon/notes\n", "Licence: \n", "Creator: \n", "Meta: \n", "\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": [" <div>\n    \t<img src=\"./coco.png\" style=\"float: left;height: 55px\">\n    \t<div style=\"height: 75px;text-align: center; padding-top:5px\">\n        <h1>\n      \tSave Model Training Progress\n        </h1>\n        <p></p>\n    \t</div>\n\t\t</div> "]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Preliminaries"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Load libraries\n", "import numpy as np\n", "from keras.datasets import imdb\n", "from keras.preprocessing.text import Tokenizer\n", "from keras import models\n", "from keras import layers\n", "from keras.callbacks import ModelCheckpoint\n", "\n", "# Set random seed\n", "np.random.seed(0)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Load IMDB Movie Review Data"]}, {"cell_type": "code", "execution_count": 8, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Set the number of features we want\n", "number_of_features = 1000\n", "\n", "# Load data and target vector from movie review data\n", "(train_data, train_target), (test_data, test_target) = imdb.load_data(num_words=number_of_features)\n", "\n", "# Convert movie review data to a one-hot encoded feature matrix\n", "tokenizer = Tokenizer(num_words=number_of_features)\n", "train_features = tokenizer.sequences_to_matrix(train_data, mode='binary')\n", "test_features = tokenizer.sequences_to_matrix(test_data, mode='binary')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Create Neural Network Architecture"]}, {"cell_type": "code", "execution_count": 9, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Start neural network\n", "network = models.Sequential()\n", "\n", "# Add fully connected layer with a ReLU activation function\n", "network.add(layers.Dense(units=16, activation='relu', input_shape=(number_of_features,)))\n", "\n", "# Add fully connected layer with a ReLU activation function\n", "network.add(layers.Dense(units=16, activation='relu'))\n", "\n", "# Add fully connected layer with a sigmoid activation function\n", "network.add(layers.Dense(units=1, activation='sigmoid'))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Compile Neural Network"]}, {"cell_type": "code", "execution_count": 10, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Compile neural network\n", "network.compile(loss='binary_crossentropy', # Cross-entropy\n", "                optimizer='rmsprop', # Root Mean Square Propagation\n", "                metrics=['accuracy']) # Accuracy performance metric"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Save Training Progress After Each Epoch\n", "\n", "After every epoch `ModelCheckpoint` saves a model to the location specified by the `filepath` parameter. If we include only a filename (e.g. `models.hdf5`) that file will be overridden with the latest model every epoch. If we only wanted to save the best model according to the performance of some loss function, we can set `save_best_only=True` and `monitor='val_loss'` to not override a file if the model has a worse test loss than the previous model. Alternatively, we can save every epoch's model as its own file by including the epoch number and test loss score into the filename itself. For example if we set `filepath` to `model_{epoch:02d}_{val_loss:.2f}.hdf5`, the name of the file containing the model saved after the 11th epoch with a test loss value of 0.33 would be `model_10_0.35.hdf5` (notice that the epoch number if 0-indexed)."]}, {"cell_type": "code", "execution_count": 11, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Set callback functions to early stop training and save the best model so far\n", "checkpoint = [ModelCheckpoint(filepath='models.hdf5')]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Train Neural Network"]}, {"cell_type": "code", "execution_count": 12, "metadata": {"collapsed": true}, "outputs": [], "source": ["# Train neural network\n", "history = network.fit(train_features, # Features\n", "                      train_target, # Target vector\n", "                      epochs=3, # Number of epochs\n", "                      callbacks=checkpoint, # Checkpoint\n", "                      verbose=0, # No output\n", "                      batch_size=100, # Number of observations per batch\n", "                      validation_data=(test_features, test_target)) # Data for evaluation"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 2}